import numpy as np
import socket
import wave
import time
import paho.mqtt.client as mqtt
import threading
from basic_pitch.inference import predict
import pathlib
import json
import librosa, soundfile as sf

# Settings (need to shift up to 22khz or 44.1khz since shifting from ADC to I2S)
MQTT_BROKER = "192.168.0.99"   # same broker as ESP32, this is running on Raspberry Pi. Can run any MQTT Broker.
UDP_PORT = 5005
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2
CHANNELS = 1

# Globals
recording = False
frames = []
sock = None
udp_thread = None
wav_filename = None
mqtt_client = None


# Set up thread for UDP audio capture
# We can shift this to TCP if necessary and if there is sufficient overhead available on the ESP32
def udp_listener():
    global recording, frames, sock
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    sock.bind(("0.0.0.0", UDP_PORT))
    sock.settimeout(1.0)
    print(f"[UDP] Listening on port {UDP_PORT}...")

    while recording:
        try:
            data, addr = sock.recvfrom(4096)
            frames.append(data)
        except socket.timeout:
            pass
    sock.close()


# MQTT start and stop recording commands

def on_message(client, userdata, msg):
    global recording, frames, udp_thread, wav_filename
    topic, payload = msg.topic, msg.payload.decode()
    print(f"[MQTT] {topic} -> {payload}")

    try:
        message = json.loads(payload)
    except json.JSONDecodeError:
        print("[WARN] Non-JSON message, ignoring")
        return

    if message.get("type") == "control":
        command = message.get("command")
        if command == "start_recording" and not recording:
            recording = True
            frames = []
            wav_filename = f"recording_{int(time.time())}.wav"
            udp_thread = threading.Thread(target=udp_listener, daemon=True)
            udp_thread.start()
            print("[INFO] Recording started")

        elif command == "stop_recording" and recording:
            recording = False
            udp_thread.join()
            print("[INFO] Recording stopped")

            with wave.open(wav_filename, "wb") as wf:
                wf.setnchannels(CHANNELS)
                wf.setsampwidth(BYTES_PER_SAMPLE)
                wf.setframerate(SAMPLE_RATE)
                wf.writeframes(b"".join(frames))
            print(f"[INFO] Saved {wav_filename}")

            run_basic_pitch(wav_filename)

# Sends message to the ESP32 on completion of processing .wav file with the number of notes processed 
def publish_feedback(client, wav_file, cleaned_notes, tempo=None, loop_length=None):
    feedback = {
        "type": "feedback",
        "status": "processed",
        "file": pathlib.Path(wav_file).name,
        "notes_count": len(cleaned_notes),
    }

    # Add optional fields if available. These are put here for future additions of tap tempo and setting loop
    #   length via device buttons, touch screen, or voice command
    if tempo is not None:
        feedback["tempo"] = tempo
    if loop_length is not None:
        feedback["loop_length"] = loop_length

    # Publish to Raspberry Pi feedback channel, the ESP32 should be looking for these messages.
    client.publish("rpi/feedback", json.dumps(feedback))
    print(f"[INFO] Published feedback: {feedback}")

# Audio Preprocessing to clean up the .wav file. Shouldn't need to resample in the future,
#   however trimming and normalizing should still help with obtaining a better AMT result
def preprocess_audio(input_wav, output_wav, target_sr=22050):
    y, sr = librosa.load(input_wav, sr=None)
    y = librosa.effects.preemphasis(y, coef=0.97)
    y = librosa.util.normalize(y)
    y, _ = librosa.effects.trim(y, top_db=25)
    if sr != target_sr:
        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)
        sr = target_sr
    sf.write(output_wav, y, sr)
    return output_wav


# Note cleaning. The midi file generated by basic-pitch might have some artifacting, including excessively short notes,
#   "ghost notes" that are not really part of the song but noise that triggered detection, and might need some quantization.
#   Provides 1/8 note quantization, a minimum amplitude, and a minimum duration of notes.
def clean_notes(
    note_events,
    min_duration=0.02,
    min_amp=0.2,
    velocity_range=(20, 127),
    quantize_grid=0.125
):
    cleaned = []
    last_note_by_pitch = {}
    for n in note_events:
        try:
            start_time, end_time, pitch, amplitude = n
        except Exception:
            continue

        dur = end_time - start_time
        if dur < min_duration or amplitude < min_amp:
            continue

        if quantize_grid:
            start_time = round(start_time / quantize_grid) * quantize_grid
            end_time = round(end_time / quantize_grid) * quantize_grid
            if end_time <= start_time:
                end_time = start_time + quantize_grid

        velocity = int(np.interp(np.sqrt(amplitude), [0, 1], velocity_range))

        if pitch in last_note_by_pitch:
            last_note = last_note_by_pitch[pitch]
            if abs(start_time - last_note['end']) <= quantize_grid / 2:
                last_note['end'] = end_time
                last_note['velocity'] = max(last_note['velocity'], velocity)
                continue

        note = {
            "start": float(start_time),
            "end": float(end_time),
            "pitch": int(pitch),
            "velocity": velocity
        }
        cleaned.append(note)
        last_note_by_pitch[pitch] = note
    return cleaned


# Run AMT pipeline, including preprocessing, AMT, and cleaning the midi output.
#   Also generates JSON output to a file and via MQTT of the results.
def run_basic_pitch(wav_file):
    print(f"[INFO] Running Basic Pitch pipeline on {wav_file}...")

    preprocessed_file = preprocess_audio(
        wav_file, wav_file.replace(".wav", "_preprocessed.wav")
    )

    model_output, midi_data, note_events = predict(preprocessed_file)
    cleaned_notes = clean_notes(note_events)

    # Save MIDI file
    midi_file = pathlib.Path(wav_file).with_suffix(".mid")
    midi_data.write(str(midi_file))

    # Save JSON file
    json_file = pathlib.Path(wav_file).with_suffix(".json")
    with open(json_file, "w") as f:
        json.dump(cleaned_notes, f, indent=2)

    print(f"[INFO] MIDI: {midi_file}, JSON: {json_file}")

    # Publish analysis via MQTT
    result = {
        "type": "analysis_result",
        "source_file": pathlib.Path(wav_file).name,
        "midi_file": str(midi_file),
        "json_file": str(json_file),
        "notes": cleaned_notes
    }
    mqtt_client.publish("rpi/analysis_result", json.dumps(result))

    # Publish summary feedback, still need to implement tempo and loop length
    publish_feedback(
        mqtt_client,
        wav_file,
        cleaned_notes,
        tempo=120,       # Placeholder for tempo
        loop_length=4    # Placeholder for loop length
    )
    
# MQTT client setup
mqtt_client = mqtt.Client()
mqtt_client.on_message = on_message
mqtt_client.connect(MQTT_BROKER, 1883, 60)
mqtt_client.subscribe("esp32/audio_control")

print("[INFO] Ready. Waiting for ESP32 MQTT commands...")
mqtt_client.loop_start()

try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    print("[INFO] Exiting cleanly...")
    mqtt_client.loop_stop()
    mqtt_client.disconnect()